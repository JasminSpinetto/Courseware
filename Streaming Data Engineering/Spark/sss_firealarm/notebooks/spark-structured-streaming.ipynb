{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Structured Streaming - Demo\n",
    "## Fire alarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://4afb58aa99c6:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff0d7a09430>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "import io\n",
    "from pyspark.sql.functions import *\n",
    "import time\n",
    "import json\n",
    "import struct\n",
    "import requests \n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1,org.apache.spark:spark-streaming-kafka-0-10_2.11:2.4.5,org.apache.kafka:kafka-clients:2.6.0 pyspark-shell'\n",
    "                                    \n",
    "spark = (SparkSession.builder \n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"test\")\n",
    "    .getOrCreate()\n",
    "        )\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set up the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoke_topic = 'SmokeSensorEvent'\n",
    "temperature_topic = 'TemperatureSensorEvent'\n",
    "servers = \"kafka:9092\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding spark-kafka integration\n",
    "Let's treat first kafka as a bulk source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoke_df = (spark\n",
    "  .read\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", servers)\n",
    "  .option(\"subscribe\", smoke_topic)\n",
    "  .option(\"startingOffsets\", \"earliest\")\n",
    "  .option(\"endingOffsets\", \"latest\")\n",
    "  .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "smoke_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------------+---------+------+--------------------+-------------+\n",
      "|    key|               value|           topic|partition|offset|           timestamp|timestampType|\n",
      "+-------+--------------------+----------------+---------+------+--------------------+-------------+\n",
      "|[53 31]|[7B 22 73 65 6E 7...|SmokeSensorEvent|        0|     0|2022-10-26 06:27:...|            0|\n",
      "|[53 31]|[7B 22 73 65 6E 7...|SmokeSensorEvent|        0|     1|2022-10-26 06:27:...|            0|\n",
      "|[53 31]|[7B 22 73 65 6E 7...|SmokeSensorEvent|        0|     2|2022-10-26 06:28:...|            0|\n",
      "|[53 31]|[7B 22 73 65 6E 7...|SmokeSensorEvent|        0|     3|2022-10-26 06:28:...|            0|\n",
      "|[53 31]|[7B 22 73 65 6E 7...|SmokeSensorEvent|        0|     4|2022-10-26 06:28:...|            0|\n",
      "+-------+--------------------+----------------+---------+------+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "smoke_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------------------------------------+\n",
      "|key|value                                             |\n",
      "+---+--------------------------------------------------+\n",
      "|S1 |{\"sensor\": \"S1\", \"smoke\": false, \"ts\": 1666765666}|\n",
      "|S1 |{\"sensor\": \"S1\", \"smoke\": false, \"ts\": 1666765677}|\n",
      "|S1 |{\"sensor\": \"S1\", \"smoke\": false, \"ts\": 1666765687}|\n",
      "|S1 |{\"sensor\": \"S1\", \"smoke\": false, \"ts\": 1666765697}|\n",
      "|S1 |{\"sensor\": \"S1\", \"smoke\": false, \"ts\": 1666765707}|\n",
      "+---+--------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stringified_smoke_df = smoke_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "stringified_smoke_df.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stringified_smoke_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "smoke_schema = StructType([\n",
    "    StructField(\"sensor\", StringType(), True),\n",
    "    StructField(\"smoke\", BooleanType(), True),\n",
    "    StructField(\"ts\", TimestampType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoke_df = stringified_smoke_df.select(col(\"key\").cast(\"string\"),from_json(col(\"value\"), smoke_schema).alias(\"value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- value: struct (nullable = true)\n",
      " |    |-- sensor: string (nullable = true)\n",
      " |    |-- smoke: boolean (nullable = true)\n",
      " |    |-- ts: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "smoke_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------------------+\n",
      "|sensor|smoke|                 ts|\n",
      "+------+-----+-------------------+\n",
      "|    S1|false|2022-10-26 06:27:46|\n",
      "|    S1|false|2022-10-26 06:27:57|\n",
      "|    S1|false|2022-10-26 06:28:07|\n",
      "|    S1|false|2022-10-26 06:28:17|\n",
      "|    S1|false|2022-10-26 06:28:27|\n",
      "+------+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "smoke_df.select(\"value.*\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's explore Spark Structured Streaming by example\n",
    "Please refer to [EPL fire allarm](https://github.com/emanueledellavalle/streaming-data-analytics/tree/main/codes/epl_firealarm) for the EPL version of the following queries.\n",
    "\n",
    "### Let's create the streaming Data Frames using the data in the kafka smoke topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_streaming_smoke_df = (spark\n",
    "  .readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", servers)\n",
    "  .option(\"startingOffsets\", \"earliest\")\n",
    "  .option(\"subscribe\", smoke_topic)\n",
    "  .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_streaming_smoke_df.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_streaming_smoke_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoke_sdf=(raw_streaming_smoke_df\n",
    "                      .select(from_json(col(\"value\").cast(\"string\"), smoke_schema).alias(\"value\"))\n",
    "                      .select(\"value.*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sensor: string (nullable = true)\n",
      " |-- smoke: boolean (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "smoke_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: it is not a DataFrame, you cannot directly execute an action on it. \n",
    "\n",
    "**The following cell *intetionally* gives an error**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Queries with streaming sources must be executed with writeStream.start();;\nkafka",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-2e418c6c2ee3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msmoke_sdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \"\"\"\n\u001b[0;32m--> 585\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Queries with streaming sources must be executed with writeStream.start();;\nkafka"
     ]
    }
   ],
   "source": [
    "smoke_sdf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queries with streaming sources must be registred and started with `writeStream.start()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's register and start a simple query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_query = (smoke_sdf\n",
    "    .writeStream\n",
    "    .format(\"memory\") # this is for debug purpose only! DO NOT USE IN PRODUCTION\n",
    "    .queryName(\"sinkTable\")\n",
    "    .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'f4a6e3c7-b360-4d50-a7de-ebba7db088b2',\n",
       " 'runId': '9b980aa7-99ee-4ce7-adce-f66293930111',\n",
       " 'name': 'sinkTable',\n",
       " 'timestamp': '2022-10-26T06:30:55.040Z',\n",
       " 'batchId': 0,\n",
       " 'numInputRows': 5,\n",
       " 'processedRowsPerSecond': 6.51890482398957,\n",
       " 'durationMs': {'addBatch': 212,\n",
       "  'getBatch': 22,\n",
       "  'latestOffset': 258,\n",
       "  'queryPlanning': 83,\n",
       "  'triggerExecution': 766,\n",
       "  'walCommit': 77},\n",
       " 'stateOperators': [],\n",
       " 'sources': [{'description': 'KafkaV2[Subscribe[SmokeSensorEvent]]',\n",
       "   'startOffset': None,\n",
       "   'endOffset': {'SmokeSensorEvent': {'0': 5}},\n",
       "   'numInputRows': 5,\n",
       "   'processedRowsPerSecond': 6.51890482398957}],\n",
       " 'sink': {'description': 'MemorySink', 'numOutputRows': 5}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_query.lastProgress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run the following cell to see the most recent content of the sinkTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------------------+\n",
      "|sensor|smoke|                 ts|\n",
      "+------+-----+-------------------+\n",
      "|    S1|false|2022-10-26 06:28:27|\n",
      "|    S1|false|2022-10-26 06:28:17|\n",
      "|    S1|false|2022-10-26 06:28:07|\n",
      "|    S1|false|2022-10-26 06:27:57|\n",
      "|    S1|false|2022-10-26 06:27:46|\n",
      "+------+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM sinkTable ORDER BY TS DESC\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "do not forget to stop queries that you are not using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create the streaming Data Frames for the kafka temperature topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperarture_schema = StructType([\n",
    "    StructField(\"sensor\", StringType(), True),\n",
    "    StructField(\"temperature\", DoubleType(), True),\n",
    "    StructField(\"ts\", TimestampType(), True)])\n",
    "\n",
    "raw_streaming_temperature_df = (spark\n",
    "  .readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", servers)\n",
    "  .option(\"startingOffsets\", \"earliest\")\n",
    "  .option(\"subscribe\", temperature_topic)\n",
    "  .load())\n",
    "\n",
    "temperature_sdf = (raw_streaming_temperature_df\n",
    "                      .select(from_json(col(\"value\").cast(\"string\"), temperarture_schema).alias(\"value\"))\n",
    "                      .select(\"value.*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sensor: string (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperature_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q0 - Filter\n",
    "\n",
    "The temperature events whose temperature is greater than 50 °C "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the SQL style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a logic table on top of the streaming data frame\n",
    "temperature_sdf.createTempView(\"TemperatureSensorEvent\")\n",
    "\n",
    "# write your query in SQL, register it and start it\n",
    "q0 = (spark.sql(\"select * from TemperatureSensorEvent where temperature > 50\")\n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .queryName(\"sinkTable\")\n",
    "                     .start())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's ask for the execution plan, we will compare it with cells down with the one of the query in Data Frame style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "WriteToDataSourceV2 org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@1cfd6f33\n",
      "+- Project [from_json(StructField(sensor,StringType,true), StructField(temperature,DoubleType,true), StructField(ts,TimestampType,true), cast(value#274 as string), Some(Etc/UTC)).sensor AS sensor#289, from_json(StructField(sensor,StringType,true), StructField(temperature,DoubleType,true), StructField(ts,TimestampType,true), cast(value#274 as string), Some(Etc/UTC)).temperature AS temperature#290, from_json(StructField(sensor,StringType,true), StructField(temperature,DoubleType,true), StructField(ts,TimestampType,true), cast(value#274 as string), Some(Etc/UTC)).ts AS ts#291]\n",
      "   +- Filter (from_json(StructField(sensor,StringType,true), StructField(temperature,DoubleType,true), StructField(ts,TimestampType,true), cast(value#274 as string), Some(Etc/UTC)).temperature > 50.0)\n",
      "      +- *(1) Project [key#273, value#274, topic#275, partition#276, offset#277L, timestamp#278, timestampType#279]\n",
      "         +- MicroBatchScan[key#273, value#274, topic#275, partition#276, offset#277L, timestamp#278, timestampType#279] class org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q0.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+---+\n",
      "|sensor|temperature| ts|\n",
      "+------+-----------+---+\n",
      "+------+-----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# look up the most recent results\n",
    "spark.sql(\"SELECT * FROM sinkTable ORDER BY TS DESC\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you are following carefully the instruction it should be empty because we are sending temperature aropund 20 °C.\n",
    "\n",
    "Go back to the `temperature_sensor_simulator` notebook, stop the cell that is sending temperature around 20°C and run the one that sends temperature around 55°C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+-------------------+\n",
      "|sensor|      temperature|                 ts|\n",
      "+------+-----------------+-------------------+\n",
      "|    S1|56.00886876914325|2022-10-26 06:31:46|\n",
      "+------+-----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# look up the most recent results\n",
    "spark.sql(\"SELECT * FROM sinkTable ORDER BY TS DESC\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should see results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up\n",
    "q0.stop()\n",
    "spark.catalog.dropTempView(\"TemperatureSensorEvent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The DataFrame style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "q0bis = (temperature_sdf\n",
    "                     .where(\"temperature > 50\") # you can add anything that fits in a SQL where statemente \n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .queryName(\"sinkTable\")\n",
    "                     .start())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's ask for the explanation of the plan. Comparing with the one of the SQL style, you can see that there is no difference. This is expected because the [catalyst optimizer](https://databricks.com/glossary/catalyst-optimizer) created it out of our declarations (which are semantically equivalent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "WriteToDataSourceV2 org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@483f70da\n",
      "+- Project [from_json(StructField(sensor,StringType,true), StructField(temperature,DoubleType,true), StructField(ts,TimestampType,true), cast(value#274 as string), Some(Etc/UTC)).sensor AS sensor#289, from_json(StructField(sensor,StringType,true), StructField(temperature,DoubleType,true), StructField(ts,TimestampType,true), cast(value#274 as string), Some(Etc/UTC)).temperature AS temperature#290, from_json(StructField(sensor,StringType,true), StructField(temperature,DoubleType,true), StructField(ts,TimestampType,true), cast(value#274 as string), Some(Etc/UTC)).ts AS ts#291]\n",
      "   +- Filter (from_json(StructField(sensor,StringType,true), StructField(temperature,DoubleType,true), StructField(ts,TimestampType,true), cast(value#274 as string), Some(Etc/UTC)).temperature > 50.0)\n",
      "      +- *(1) Project [key#273, value#274, topic#275, partition#276, offset#277L, timestamp#278, timestampType#279]\n",
      "         +- MicroBatchScan[key#273, value#274, topic#275, partition#276, offset#277L, timestamp#278, timestampType#279] class org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q0bis.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+-------------------+\n",
      "|sensor|      temperature|                 ts|\n",
      "+------+-----------------+-------------------+\n",
      "|    S1|54.22509249269572|2022-10-26 06:32:06|\n",
      "|    S1|56.26551525029317|2022-10-26 06:31:56|\n",
      "|    S1|56.00886876914325|2022-10-26 06:31:46|\n",
      "+------+-----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM sinkTable ORDER BY TS DESC\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "q0bis.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: there was no need to\n",
    "> * create a logic table on top of the streaming data frame with `temperature_sdf.createTempView(\"TemperatureSensorEvent\")`\n",
    "> * drop such a logic table with `spark.catalog.dropTempView(\"TemperatureSensorEvent\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 - Avg\n",
    "\n",
    "the average of all the temperature observation for each sensor up to the last event received"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the SQL sytyle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a logic table on top of the streaming data frame\n",
    "temperature_sdf.createTempView(\"TemperatureSensorEvent\") # this time we will not clean it up, because we use it in the next queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: the following query gives *intentionally* an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark;;\nAggregate [SENSOR#289], [SENSOR#289, avg(temperature#290) AS avg(temperature)#432]\n+- SubqueryAlias temperaturesensorevent\n   +- Project [value#287.sensor AS sensor#289, value#287.temperature AS temperature#290, value#287.ts AS ts#291]\n      +- Project [from_json(StructField(sensor,StringType,true), StructField(temperature,DoubleType,true), StructField(ts,TimestampType,true), cast(value#274 as string), Some(Etc/UTC)) AS value#287]\n         +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@4439d11c, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@54fe14de, org.apache.spark.sql.util.CaseInsensitiveStringMap@24f684c4, [key#273, value#274, topic#275, partition#276, offset#277L, timestamp#278, timestampType#279], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@14a73345,kafka,List(),None,List(),None,Map(startingOffsets -> earliest, subscribe -> TemperatureSensorEvent, kafka.bootstrap.servers -> kafka:9092),None), kafka, [key#266, value#267, topic#268, partition#269, offset#270L, timestamp#271, timestampType#272]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-2bfd283c8455>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# write your query in SQL, register it and start it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m q1 = (spark.sql(query_string)\n\u001b[0m\u001b[1;32m      9\u001b[0m                      \u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                      \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1209\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1212\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark;;\nAggregate [SENSOR#289], [SENSOR#289, avg(temperature#290) AS avg(temperature)#432]\n+- SubqueryAlias temperaturesensorevent\n   +- Project [value#287.sensor AS sensor#289, value#287.temperature AS temperature#290, value#287.ts AS ts#291]\n      +- Project [from_json(StructField(sensor,StringType,true), StructField(temperature,DoubleType,true), StructField(ts,TimestampType,true), cast(value#274 as string), Some(Etc/UTC)) AS value#287]\n         +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@4439d11c, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@54fe14de, org.apache.spark.sql.util.CaseInsensitiveStringMap@24f684c4, [key#273, value#274, topic#275, partition#276, offset#277L, timestamp#278, timestampType#279], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@14a73345,kafka,List(),None,List(),None,Map(startingOffsets -> earliest, subscribe -> TemperatureSensorEvent, kafka.bootstrap.servers -> kafka:9092),None), kafka, [key#266, value#267, topic#268, partition#269, offset#270L, timestamp#271, timestampType#272]\n"
     ]
    }
   ],
   "source": [
    "query_string = \"\"\"\n",
    "SELECT SENSOR, AVG(temperature) \n",
    "FROM TemperatureSensorEvent\n",
    "GROUP BY SENSOR\n",
    "\"\"\"\n",
    "\n",
    "# write your query in SQL, register it and start it\n",
    "q1 = (spark.sql(query_string)\n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .queryName(\"sinkTable\")\n",
    "                     .start())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **append output mode** (i.e., the default one) is not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark, we need to use the **complete output mode**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_string = \"\"\"\n",
    "SELECT SENSOR, AVG(temperature) \n",
    "FROM TemperatureSensorEvent\n",
    "GROUP BY SENSOR\n",
    "\"\"\"\n",
    "\n",
    "# write your query in SQL, register it and start it\n",
    "q1 = (spark.sql(query_string)\n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .outputMode(\"complete\") # <-- CHANGE HERE\n",
    "                     .queryName(\"sinkTable\")\n",
    "                     .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+\n",
      "|SENSOR| avg(temperature)|\n",
      "+------+-----------------+\n",
      "|    S1|53.74661986795754|\n",
      "+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# look up the most recent results\n",
    "spark.sql(\"SELECT * FROM sinkTable\").show() # without ORDER BY TS DESC because the result in the table is already only the most recent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: if the cell above gives an empty result, wait 10 seconds and run it again. The very first excution may take time, expecially if you have already ingested many temperature events in kafka. Here we are querying the sink table and it may be empty because the first execution is still running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up\n",
    "q1.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The DataFrame style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your query in SQL, register it and start it\n",
    "q1bis = (temperature_sdf \n",
    "                     .groupBy(\"sensor\")\n",
    "                     .avg()\n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .outputMode(\"complete\") \n",
    "                     .queryName(\"sinkTable\")\n",
    "                     .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+\n",
      "|sensor| avg(temperature)|\n",
      "+------+-----------------+\n",
      "|    S1|53.76653609767246|\n",
      "+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# look up the most recent results\n",
    "spark.sql(\"SELECT * FROM sinkTable\").show() # woithout ORDER BY TS DESC because the result in the table is already only the most recent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up\n",
    "q1bis.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 - Logical Sliding Window\n",
    "\n",
    "The average temperature observed by each sensor in the last 4 seconds\n",
    "\n",
    "MEMO: the average should change as soon as the receive a new event\n",
    "\n",
    "**Not supported**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 - Logical Tumbling Window\n",
    "\n",
    "The average temperature of the last 30 seconds every 30 seconds (was 4 seconds in EPL)\n",
    "\n",
    "NOTE: this query is not possibile in the SQL style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "q3 = (temperature_sdf\n",
    "                  .groupBy(window(\"TS\", \"30 seconds\"),\"SENSOR\")\n",
    "                  .avg(\"TEMPERATURE\")\n",
    "                  .writeStream\n",
    "                  .outputMode(\"complete\")\n",
    "                  .format(\"memory\")\n",
    "                  .queryName(\"sinkTable\")\n",
    "                  .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+------+------------------+\n",
      "|window                                    |SENSOR|avg(TEMPERATURE)  |\n",
      "+------------------------------------------+------+------------------+\n",
      "|[2022-10-26 07:15:30, 2022-10-26 07:16:00]|S1    |55.32720389345427 |\n",
      "|[2022-10-26 06:56:00, 2022-10-26 06:56:30]|S1    |54.75417122529101 |\n",
      "|[2022-10-26 06:55:30, 2022-10-26 06:56:00]|S1    |55.09520890669898 |\n",
      "|[2022-10-26 06:55:00, 2022-10-26 06:55:30]|S1    |55.310080691870475|\n",
      "|[2022-10-26 06:54:30, 2022-10-26 06:55:00]|S1    |55.164726288446275|\n",
      "+------------------------------------------+------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM sinkTable ORDER BY window DESC\").show(5,False) # NOTE: here we order by window instead of ordering by timestamp# window instead of timestamp, again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "q3.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4 - Physical Sliding Window\n",
    "\n",
    "The moving average of the last 4 temperature events\n",
    "\n",
    "**Not supported**\n",
    "\n",
    "## Q5 - Physical Tumbling Window\n",
    "\n",
    "The moving average of the last 4 temperature events every 4 events \n",
    "\n",
    "**Not supported**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6 - Logical Hopping Window\n",
    "\n",
    "The average temperature of the last 1 minute (was 4 seconds in EPL) every 30 seconds (was 2 seconds in EPL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "q6 = (temperature_sdf\n",
    "      .groupBy(window(\"TS\", \"1 minutes\", \"30 seconds\"),\"SENSOR\")\n",
    "      .avg(\"TEMPERATURE\")\n",
    "      .writeStream\n",
    "      .outputMode(\"complete\")\n",
    "      .format(\"memory\")\n",
    "      .queryName(\"sinkTable\")\n",
    "      .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+------+------------------+\n",
      "|window                                    |SENSOR|avg(TEMPERATURE)  |\n",
      "+------------------------------------------+------+------------------+\n",
      "|[2022-10-26 07:15:30, 2022-10-26 07:16:30]|S1    |56.68803759086834 |\n",
      "|[2022-10-26 07:15:00, 2022-10-26 07:16:00]|S1    |56.68803759086834 |\n",
      "|[2022-10-26 06:56:00, 2022-10-26 06:57:00]|S1    |54.75417122529101 |\n",
      "|[2022-10-26 06:55:30, 2022-10-26 06:56:30]|S1    |54.95879383413579 |\n",
      "|[2022-10-26 06:55:00, 2022-10-26 06:56:00]|S1    |55.202644799284734|\n",
      "|[2022-10-26 06:54:30, 2022-10-26 06:55:30]|S1    |55.23740349015838 |\n",
      "+------------------------------------------+------+------------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM sinkTable ORDER BY window DESC\").show(6,False) # NOTE: here we order by window instead of ordering by timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "q6.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7 - Stream-to-Stream Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In EPL, at this point we moved on to the pattern matching part required to satisfy the information need, i.e., \"find every smoke event followed by a temperature event whose temperature is above 50 °C within 2 minutes.\"\n",
    "\n",
    "Spark Structured Streaming does not support the EPL's operator `->` (that reads as *followed by*. We need to use a stream-to-stream join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_minute_smoke_events = (smoke_sdf\n",
    "                .where(\"smoke = True\")\n",
    "                .withColumnRenamed(\"sensor\",\"sensorSmoke\")\n",
    "                .withColumnRenamed(\"ts\",\"tsSmoke\")\n",
    "               )\n",
    "\n",
    "last_minute_high_temperature_events = (temperature_sdf\n",
    "                .where(\"temperature > 50\")\n",
    "                .withColumnRenamed(\"sensor\",\"sensorTemp\")\n",
    "                .withColumnRenamed(\"ts\",\"tsTemp\")\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join with event-time constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_sdf = (last_minute_smoke_events.join(\n",
    "  last_minute_high_temperature_events, expr(\"\"\"\n",
    "    (sensorTemp == sensorSmoke) AND\n",
    "    (tsTemp > tsSmoke ) AND\n",
    "    (tsTemp < tsSmoke + interval 2 minute )\n",
    "    \"\"\"\n",
    "    )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "q7 = (join_sdf\n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .queryName(\"sinkTable\")\n",
    "                     .start())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT** To detect fire, run the appropriate cells in the data generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-------+----------+-----------+------+\n",
      "|sensorSmoke|smoke|tsSmoke|sensorTemp|temperature|tsTemp|\n",
      "+-----------+-----+-------+----------+-----------+------+\n",
      "+-----------+-----+-------+----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM sinkTable ORDER BY tsTemp DESC\").show(20,False) # note, I change ts in tsTemp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are following carefully the instructions the answer should be empty because we are sending temperature around 55°C, but there is no `smoke==true`, yet.\n",
    "\n",
    "Go to the `smoke_sensor_simulator` notebook and start sending `smoke==true`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-------------------+----------+------------------+-------------------+\n",
      "|sensorSmoke|smoke|tsSmoke            |sensorTemp|temperature       |tsTemp             |\n",
      "+-----------+-----+-------------------+----------+------------------+-------------------+\n",
      "|S1         |true |2022-10-26 07:17:14|S1        |54.407203182829065|2022-10-26 07:17:18|\n",
      "+-----------+-----+-------------------+----------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM sinkTable ORDER BY tsTemp DESC\").show(20,False) # note, I change ts in tsTemp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's have a look to the progresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"5d195f3f-a119-46ab-9a63-01682dd7369a\",\n",
      "    \"runId\": \"23d75bd5-ca0b-430f-8c63-d3669fcdb19b\",\n",
      "    \"name\": \"sinkTable\",\n",
      "    \"timestamp\": \"2022-10-26T07:19:30.340Z\",\n",
      "    \"batchId\": 12,\n",
      "    \"numInputRows\": 2,\n",
      "    \"inputRowsPerSecond\": 0.17725782150137376,\n",
      "    \"processedRowsPerSecond\": 0.1388888888888889,\n",
      "    \"durationMs\": {\n",
      "        \"addBatch\": 14255,\n",
      "        \"getBatch\": 0,\n",
      "        \"latestOffset\": 2,\n",
      "        \"queryPlanning\": 94,\n",
      "        \"triggerExecution\": 14400,\n",
      "        \"walCommit\": 19\n",
      "    },\n",
      "    \"stateOperators\": [\n",
      "        {\n",
      "            \"numRowsTotal\": 186,\n",
      "            \"numRowsUpdated\": 2,\n",
      "            \"memoryUsedBytes\": 382496,\n",
      "            \"customMetrics\": {\n",
      "                \"loadedMapCacheHitCount\": 4800,\n",
      "                \"loadedMapCacheMissCount\": 0,\n",
      "                \"stateOnCurrentVersionSizeBytes\": 67984\n",
      "            }\n",
      "        }\n",
      "    ],\n",
      "    \"sources\": [\n",
      "        {\n",
      "            \"description\": \"KafkaV2[Subscribe[SmokeSensorEvent]]\",\n",
      "            \"startOffset\": {\n",
      "                \"SmokeSensorEvent\": {\n",
      "                    \"0\": 18\n",
      "                }\n",
      "            },\n",
      "            \"endOffset\": {\n",
      "                \"SmokeSensorEvent\": {\n",
      "                    \"0\": 19\n",
      "                }\n",
      "            },\n",
      "            \"numInputRows\": 1,\n",
      "            \"inputRowsPerSecond\": 0.08862891075068688,\n",
      "            \"processedRowsPerSecond\": 0.06944444444444445\n",
      "        },\n",
      "        {\n",
      "            \"description\": \"KafkaV2[Subscribe[TemperatureSensorEvent]]\",\n",
      "            \"startOffset\": {\n",
      "                \"TemperatureSensorEvent\": {\n",
      "                    \"0\": 174\n",
      "                }\n",
      "            },\n",
      "            \"endOffset\": {\n",
      "                \"TemperatureSensorEvent\": {\n",
      "                    \"0\": 175\n",
      "                }\n",
      "            },\n",
      "            \"numInputRows\": 1,\n",
      "            \"inputRowsPerSecond\": 0.08862891075068688,\n",
      "            \"processedRowsPerSecond\": 0.06944444444444445\n",
      "        }\n",
      "    ],\n",
      "    \"sink\": {\n",
      "        \"description\": \"MemorySink\",\n",
      "        \"numOutputRows\": 12\n",
      "    }\n",
      "}\n",
      "{'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-51852bc8d4b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq7\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlastProgress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq7\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import json\n",
    "while True:\n",
    "    print(json.dumps(q7.lastProgress, indent=4))\n",
    "    print(q7.status)\n",
    "    time.sleep(1)\n",
    "    clear_output(wait=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to interrupt the execution of the cell, prese the square icon in the bar or choose *interrupt kernel* from the *kernel* dropdown menu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This query is equivalent to the EPL pattern `every a = SmokeSensorEvent(smoke=true) -> every TemperatureSensorEvent(temperature > 50, sensor=a.sensor) where timer:within(1 min)`. \n",
    ">\n",
    "> Do not expect the same performances! It is evaluated as a relational join. Spark Structured Streaming lacks the specilized data structure of Esper.\n",
    ">\n",
    "> **It does not tame the torrent effect**, but this is expected! \n",
    ">\n",
    "> Spark Structured Streaming is a Data Stream Management System meant to tame *flow that you cannot stop*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even id Q8 consumes Q7 results, we can stop Q7 because we only need the streaming Data Frame `join_sdf`. We do not need Q7 to write its result in the in memory table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "q7.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q8 - Count FireEvent\n",
    "\n",
    "we are very close to the solution of the running example, we \"just\" need to count the number of events generated by the previous query over an hopping window of 1 minutes that slides every 30 seconds (was a sliding window of 10 secondsin EPL). \n",
    "\n",
    "So let's count the results of Q7. \n",
    "\n",
    "**NOTE**: the following queries give *intentionally* errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Join between two streaming DataFrames/Datasets is not supported in Complete output mode, only in Append output mode;;\nJoin Inner, (((sensorTemp#16516 = sensorSmoke#16508) AND (tsTemp#16520 > tsSmoke#16512)) AND (tsTemp#16520 < cast(tsSmoke#16512 + 2 minutes as timestamp)))\n:- Project [sensorSmoke#16508, smoke#216, ts#217 AS tsSmoke#16512]\n:  +- Project [sensor#215 AS sensorSmoke#16508, smoke#216, ts#217]\n:     +- Filter (smoke#216 = true)\n:        +- Project [value#213.sensor AS sensor#215, value#213.smoke AS smoke#216, value#213.ts AS ts#217]\n:           +- Project [from_json(StructField(sensor,StringType,true), StructField(smoke,BooleanType,true), StructField(ts,TimestampType,true), cast(value#200 as string), Some(Etc/UTC)) AS value#213]\n:              +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@a96be10, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@3f675c30, org.apache.spark.sql.util.CaseInsensitiveStringMap@7098eff9, [key#199, value#200, topic#201, partition#202, offset#203L, timestamp#204, timestampType#205], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@14a73345,kafka,List(),None,List(),None,Map(startingOffsets -> earliest, subscribe -> SmokeSensorEvent, kafka.bootstrap.servers -> kafka:9092),None), kafka, [key#192, value#193, topic#194, partition#195, offset#196L, timestamp#197, timestampType#198]\n+- Project [sensorTemp#16516, temperature#290, ts#291 AS tsTemp#16520]\n   +- Project [sensor#289 AS sensorTemp#16516, temperature#290, ts#291]\n      +- Filter (temperature#290 > cast(50 as double))\n         +- Project [value#287.sensor AS sensor#289, value#287.temperature AS temperature#290, value#287.ts AS ts#291]\n            +- Project [from_json(StructField(sensor,StringType,true), StructField(temperature,DoubleType,true), StructField(ts,TimestampType,true), cast(value#274 as string), Some(Etc/UTC)) AS value#287]\n               +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@4439d11c, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@54fe14de, org.apache.spark.sql.util.CaseInsensitiveStringMap@24f684c4, [key#273, value#274, topic#275, partition#276, offset#277L, timestamp#278, timestampType#279], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@14a73345,kafka,List(),None,List(),None,Map(startingOffsets -> earliest, subscribe -> TemperatureSensorEvent, kafka.bootstrap.servers -> kafka:9092),None), kafka, [key#266, value#267, topic#268, partition#269, offset#270L, timestamp#271, timestampType#272]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-abe36acef28b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m q8 = (join_sdf\n\u001b[0m\u001b[1;32m      2\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tsTemp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"1 minutes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"30 seconds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"sensorTemp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0moutputMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"complete\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1209\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1212\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Join between two streaming DataFrames/Datasets is not supported in Complete output mode, only in Append output mode;;\nJoin Inner, (((sensorTemp#16516 = sensorSmoke#16508) AND (tsTemp#16520 > tsSmoke#16512)) AND (tsTemp#16520 < cast(tsSmoke#16512 + 2 minutes as timestamp)))\n:- Project [sensorSmoke#16508, smoke#216, ts#217 AS tsSmoke#16512]\n:  +- Project [sensor#215 AS sensorSmoke#16508, smoke#216, ts#217]\n:     +- Filter (smoke#216 = true)\n:        +- Project [value#213.sensor AS sensor#215, value#213.smoke AS smoke#216, value#213.ts AS ts#217]\n:           +- Project [from_json(StructField(sensor,StringType,true), StructField(smoke,BooleanType,true), StructField(ts,TimestampType,true), cast(value#200 as string), Some(Etc/UTC)) AS value#213]\n:              +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@a96be10, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@3f675c30, org.apache.spark.sql.util.CaseInsensitiveStringMap@7098eff9, [key#199, value#200, topic#201, partition#202, offset#203L, timestamp#204, timestampType#205], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@14a73345,kafka,List(),None,List(),None,Map(startingOffsets -> earliest, subscribe -> SmokeSensorEvent, kafka.bootstrap.servers -> kafka:9092),None), kafka, [key#192, value#193, topic#194, partition#195, offset#196L, timestamp#197, timestampType#198]\n+- Project [sensorTemp#16516, temperature#290, ts#291 AS tsTemp#16520]\n   +- Project [sensor#289 AS sensorTemp#16516, temperature#290, ts#291]\n      +- Filter (temperature#290 > cast(50 as double))\n         +- Project [value#287.sensor AS sensor#289, value#287.temperature AS temperature#290, value#287.ts AS ts#291]\n            +- Project [from_json(StructField(sensor,StringType,true), StructField(temperature,DoubleType,true), StructField(ts,TimestampType,true), cast(value#274 as string), Some(Etc/UTC)) AS value#287]\n               +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@4439d11c, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@54fe14de, org.apache.spark.sql.util.CaseInsensitiveStringMap@24f684c4, [key#273, value#274, topic#275, partition#276, offset#277L, timestamp#278, timestampType#279], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@14a73345,kafka,List(),None,List(),None,Map(startingOffsets -> earliest, subscribe -> TemperatureSensorEvent, kafka.bootstrap.servers -> kafka:9092),None), kafka, [key#266, value#267, topic#268, partition#269, offset#270L, timestamp#271, timestampType#272]\n"
     ]
    }
   ],
   "source": [
    "q8 = (join_sdf\n",
    "            .groupBy(window(\"tsTemp\", \"1 minutes\", \"30 seconds\"),\"sensorTemp\")\n",
    "            .count()\n",
    "            .writeStream\n",
    "            .outputMode(\"complete\")\n",
    "            .format(\"memory\")\n",
    "            .queryName(\"sinkTable\") \n",
    "            .start())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, join between two streaming DataFrames/Datasets is not supported in Complete output mode, only in Append output mode. So let's try to use append mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark;;\nAggregate [window#87092, sensorTemp#16516], [window#87092 AS window#87083, sensorTemp#16516, count(1) AS count#87091L]\n+- Filter ((tsTemp#16520 >= window#87092.start) AND (tsTemp#16520 < window#87092.end))\n   +- Expand [ArrayBuffer(named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(tsTemp#16520, TimestampType, LongType) - 0) as double) / cast(30000000 as double))) as double) = (cast((precisetimestampconversion(tsTemp#16520, TimestampType, LongType) - 0) as double) / cast(30000000 as double))) THEN (CEIL((cast((precisetimestampconversion(tsTemp#16520, TimestampType, LongType) - 0) as double) / cast(30000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(tsTemp#16520, TimestampType, LongType) - 0) as double) / cast(30000000 as double))) END + cast(0 as bigint)) - cast(2 as bigint)) * 30000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(tsTemp#16520, TimestampType, LongType) - 0) as double) / cast(30000000 as double))) as double) = (cast((precisetimestampconversion(tsTemp#16520, TimestampType, LongType) - 0) as double) / cast(30000000 as double))) THEN (CEIL((cast((precisetimestampconversion(tsTemp#16520, TimestampType, LongType) - 0) as double) / cast(30000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(tsTemp#16520, TimestampType, LongType) - 0) as double) / cast(30000000 as double))) END + cast(0 as bigint)) - cast(2 as bigint)) * 30000000) + 0) + 60000000), LongType, TimestampType)), sensorSmoke#16508, smoke#216, tsSmoke#16512, sensorTemp#16516, temperature#290, tsTemp#16520), ArrayBuffer(named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(tsTemp#16520, TimestampType, LongType) - 0) as double) / cast(30000000 as double))) as double) = (cast((precisetimestampconversion(tsTemp#16520, TimestampType, LongType) - 0) as double) / cast(30000000 as double))) THEN (CEIL((cast((precisetimestampconversion(tsTemp#16520, TimestampType, LongType) - 0) as double) / cast(30000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(tsTemp#16520, TimestampType, LongType) - 0) as double) / cast(30000000 as double))) END + cast(1 as bigint)) - cast(2 as bigint)) * 30000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(tsTemp#16520, TimestampType, LongType) - 0) as double) / cast(30000000 as double))) as double) = (cast((precisetimestampconversion(tsTemp#16520, TimestampType, LongType) - 0) as double) / cast(30000000 as double))) THEN (CEIL((cast((precisetimestampconversion(tsTemp#16520, TimestampType, LongType) - 0) as double) / cast(30000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(tsTemp#16520, TimestampType, LongType) - 0) as double) / cast(30000000 as double))) END + cast(1 as bigint)) - cast(2 as bigint)) * 30000000) + 0) + 60000000), LongType, TimestampType)), sensorSmoke#16508, smoke#216, tsSmoke#16512, sensorTemp#16516, temperature#290, tsTemp#16520)], [window#87092, sensorSmoke#16508, smoke#216, tsSmoke#16512, sensorTemp#16516, temperature#290, tsTemp#16520]\n      +- Join Inner, (((sensorTemp#16516 = sensorSmoke#16508) AND (tsTemp#16520 > tsSmoke#16512)) AND (tsTemp#16520 < cast(tsSmoke#16512 + 2 minutes as timestamp)))\n         :- Project [sensorSmoke#16508, smoke#216, ts#217 AS tsSmoke#16512]\n         :  +- Project [sensor#215 AS sensorSmoke#16508, smoke#216, ts#217]\n         :     +- Filter (smoke#216 = true)\n         :        +- Project [value#213.sensor AS sensor#215, value#213.smoke AS smoke#216, value#213.ts AS ts#217]\n         :           +- Project [from_json(StructField(sensor,StringType,true), StructField(smoke,BooleanType,true), StructField(ts,TimestampType,true), cast(value#200 as string), Some(Etc/UTC)) AS value#213]\n         :              +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@a96be10, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@3f675c30, org.apache.spark.sql.util.CaseInsensitiveStringMap@7098eff9, [key#199, value#200, topic#201, partition#202, offset#203L, timestamp#204, timestampType#205], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@14a73345,kafka,List(),None,List(),None,Map(startingOffsets -> earliest, subscribe -> SmokeSensorEvent, kafka.bootstrap.servers -> kafka:9092),None), kafka, [key#192, value#193, topic#194, partition#195, offset#196L, timestamp#197, timestampType#198]\n         +- Project [sensorTemp#16516, temperature#290, ts#291 AS tsTemp#16520]\n            +- Project [sensor#289 AS sensorTemp#16516, temperature#290, ts#291]\n               +- Filter (temperature#290 > cast(50 as double))\n                  +- Project [value#287.sensor AS sensor#289, value#287.temperature AS temperature#290, value#287.ts AS ts#291]\n                     +- Project [from_json(StructField(sensor,StringType,true), StructField(temperature,DoubleType,true), StructField(ts,TimestampType,true), cast(value#274 as string), Some(Etc/UTC)) AS value#287]\n                        +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@4439d11c, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@54fe14de, org.apache.spark.sql.util.CaseInsensitiveStringMap@24f684c4, [key#273, value#274, topic#275, partition#276, offset#277L, timestamp#278, timestampType#279], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@14a73345,kafka,List(),None,List(),None,Map(startingOffsets -> earliest, subscribe -> TemperatureSensorEvent, kafka.bootstrap.servers -> kafka:9092),None), kafka, [key#266, value#267, topic#268, partition#269, offset#270L, timestamp#271, timestampType#272]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-a25994c9d67a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m q8 = (join_sdf\n\u001b[0m\u001b[1;32m      2\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tsTemp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"1 minutes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"30 seconds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"sensorTemp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0moutputMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"append\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# <-- CHANGE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1209\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1212\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark;;\nAggregate [window#87092, sensorTemp#16516], [window#87092 AS window#87083, sensorTemp#16516, count(1) AS count#87091L]\n+- Filter ((tsTemp#16520 >= window#87092.start) AND (tsTemp#16520 < window#87092.end))\n   +- Expand [ArrayBuffer(named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(tsTemp#16520, TimestampType, LongType) - 0) as double) / cast(30000000 as double))) as double) = (cast((precisetimestampconversion(tsTemp#16520, TimestampType, LongType) - 0) as double) / cast(30000000 as double))) THEN (CEIL((cast((precisetimestampconversion(tsTemp#16520, TimestampType, LongType) - 0) as double) / cast(30000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(tsTemp#16520, TimestampType, LongType) - 0) as double) / cast(30000000 as double))) END + cast(0 as bigint)) - cast(2 as bigint)) * 30000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(tsTemp#16520, TimestampType, LongType) - 0) as double) / cast(30000000 as double))) as double) = (cast((precisetimestampconversion(tsTemp#16520, TimestampType, LongType) - 0) as double) / cast(30000000 as double))) THEN (CEIL((cast((precisetimestampconversion(tsTemp#16520, TimestampType, LongType) - 0) as double) / cast(30000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(tsTemp#16520, TimestampType, LongType) - 0) as double) / cast(30000000 as double))) END + cast(0 as bigint)) - cast(2 as bigint)) * 30000000) + 0) + 60000000), LongType, TimestampType)), sensorSmoke#16508, smoke#216, tsSmoke#16512, sensorTemp#16516, temperature#290, tsTemp#16520), ArrayBuffer(named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(tsTemp#16520, TimestampType, LongType) - 0) as double) / cast(30000000 as double))) as double) = (cast((precisetimestampconversion(tsTemp#16520, TimestampType, LongType) - 0) as double) / cast(30000000 as double))) THEN (CEIL((cast((precisetimestampconversion(tsTemp#16520, TimestampType, LongType) - 0) as double) / cast(30000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(tsTemp#16520, TimestampType, LongType) - 0) as double) / cast(30000000 as double))) END + cast(1 as bigint)) - cast(2 as bigint)) * 30000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(tsTemp#16520, TimestampType, LongType) - 0) as double) / cast(30000000 as double))) as double) = (cast((precisetimestampconversion(tsTemp#16520, TimestampType, LongType) - 0) as double) / cast(30000000 as double))) THEN (CEIL((cast((precisetimestampconversion(tsTemp#16520, TimestampType, LongType) - 0) as double) / cast(30000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(tsTemp#16520, TimestampType, LongType) - 0) as double) / cast(30000000 as double))) END + cast(1 as bigint)) - cast(2 as bigint)) * 30000000) + 0) + 60000000), LongType, TimestampType)), sensorSmoke#16508, smoke#216, tsSmoke#16512, sensorTemp#16516, temperature#290, tsTemp#16520)], [window#87092, sensorSmoke#16508, smoke#216, tsSmoke#16512, sensorTemp#16516, temperature#290, tsTemp#16520]\n      +- Join Inner, (((sensorTemp#16516 = sensorSmoke#16508) AND (tsTemp#16520 > tsSmoke#16512)) AND (tsTemp#16520 < cast(tsSmoke#16512 + 2 minutes as timestamp)))\n         :- Project [sensorSmoke#16508, smoke#216, ts#217 AS tsSmoke#16512]\n         :  +- Project [sensor#215 AS sensorSmoke#16508, smoke#216, ts#217]\n         :     +- Filter (smoke#216 = true)\n         :        +- Project [value#213.sensor AS sensor#215, value#213.smoke AS smoke#216, value#213.ts AS ts#217]\n         :           +- Project [from_json(StructField(sensor,StringType,true), StructField(smoke,BooleanType,true), StructField(ts,TimestampType,true), cast(value#200 as string), Some(Etc/UTC)) AS value#213]\n         :              +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@a96be10, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@3f675c30, org.apache.spark.sql.util.CaseInsensitiveStringMap@7098eff9, [key#199, value#200, topic#201, partition#202, offset#203L, timestamp#204, timestampType#205], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@14a73345,kafka,List(),None,List(),None,Map(startingOffsets -> earliest, subscribe -> SmokeSensorEvent, kafka.bootstrap.servers -> kafka:9092),None), kafka, [key#192, value#193, topic#194, partition#195, offset#196L, timestamp#197, timestampType#198]\n         +- Project [sensorTemp#16516, temperature#290, ts#291 AS tsTemp#16520]\n            +- Project [sensor#289 AS sensorTemp#16516, temperature#290, ts#291]\n               +- Filter (temperature#290 > cast(50 as double))\n                  +- Project [value#287.sensor AS sensor#289, value#287.temperature AS temperature#290, value#287.ts AS ts#291]\n                     +- Project [from_json(StructField(sensor,StringType,true), StructField(temperature,DoubleType,true), StructField(ts,TimestampType,true), cast(value#274 as string), Some(Etc/UTC)) AS value#287]\n                        +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@4439d11c, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@54fe14de, org.apache.spark.sql.util.CaseInsensitiveStringMap@24f684c4, [key#273, value#274, topic#275, partition#276, offset#277L, timestamp#278, timestampType#279], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@14a73345,kafka,List(),None,List(),None,Map(startingOffsets -> earliest, subscribe -> TemperatureSensorEvent, kafka.bootstrap.servers -> kafka:9092),None), kafka, [key#266, value#267, topic#268, partition#269, offset#270L, timestamp#271, timestampType#272]\n"
     ]
    }
   ],
   "source": [
    "q8 = (join_sdf\n",
    "            .groupBy(window(\"tsTemp\", \"1 minutes\", \"30 seconds\"),\"sensorTemp\")\n",
    "            .count()\n",
    "            .writeStream\n",
    "            .outputMode(\"append\") # <-- CHANGE HERE\n",
    "            .format(\"memory\")\n",
    "            .queryName(\"sinkTable\") \n",
    "            .start())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append output mode is not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark. Indeed, **the streaming join can create out of orders**.\n",
    "\n",
    "Let's add a watermark, then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "q8 = (join_sdf\n",
    "            .withWatermark(delayThreshold=\"2 minutes\",eventTime=\"tsTemp\") # <-- CHANGE HERE\n",
    "            .groupBy(window(\"tsTemp\", \"1 minutes\", \"30 seconds\"),\"sensorTemp\")\n",
    "            .count()\n",
    "            .writeStream\n",
    "            .outputMode(\"append\") \n",
    "            .format(\"memory\")\n",
    "            .queryName(\"sinkTable\") \n",
    "            .start())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: 2 minutes is maximum delay that the join can cause given the way we declared it. The temperature and the smoke event cannot be more than 2 minutes apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+----------+-----+\n",
      "|window                                    |sensorTemp|count|\n",
      "+------------------------------------------+----------+-----+\n",
      "|[2022-10-26 07:17:30, 2022-10-26 07:18:30]|S1        |33   |\n",
      "|[2022-10-26 07:17:00, 2022-10-26 07:18:00]|S1        |15   |\n",
      "|[2022-10-26 07:16:30, 2022-10-26 07:17:30]|S1        |3    |\n",
      "+------------------------------------------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM sinkTable ORDER BY window DESC\").show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "q8.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
